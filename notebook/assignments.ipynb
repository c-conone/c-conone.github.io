{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c51756a",
   "metadata": {},
   "source": [
    "# Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f6e78",
   "metadata": {},
   "source": [
    "## Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5657135a",
   "metadata": {},
   "source": [
    "### DataFrame Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f8210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up shell for libraries, packages, and data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "metadata = pd.read_csv('../data/meta_data.csv')\n",
    "df = pd.read_csv('../data/transaction_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a631023",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9fcdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eacd2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sku_number', 'inventory_type', 'stocking_type', 'lead_time', 'unit_price', 'manufacturing_site', 'division_code', 'transaction_date', 'order_quantity']\n"
     ]
    }
   ],
   "source": [
    "# Dataframe Formatting ~ Changing column names to follow the same naming convention\n",
    "\n",
    "# Renaming columns for \"transaction\" dataframe\n",
    "\n",
    "df.rename(columns={'sku_numb3r': 'sku_number','leadTime': 'lead_time','Division Code':'division_code','Order-Quantity':'order_quantity'}, inplace=True)\n",
    "\n",
    "# view df column names as list:\n",
    "\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a22282",
   "metadata": {},
   "source": [
    "### Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f1c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the numeric features of the transaction dataframe\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c4eb7",
   "metadata": {},
   "source": [
    " **Data Quality Issues (Oddities & Anomolies):**\n",
    "\n",
    "* Missing Values- `manufacturing_site`, `division_code`, and `stocking_type` contain several missing values (NaN & NA).\n",
    "\n",
    "* Outliers in `unit_price` - column contains at least one observation with a negative value; most values in the data set average around ~$800, there is at least one obervation with unit price of -$994.\n",
    "\n",
    "* Outliers in `order_quantity` - most quantites are in double-digits, at least one observation has a low order number (3).\n",
    "\n",
    "\n",
    "**Changes and Recommendations:**\n",
    "\n",
    "* Remove NaN values and \"NA\" strings to preserve data integrity.\n",
    "* Apply filter to `unit_price` by removing negative values and filter `order_quantity` to only reflect observations with a value of 10 or higher to remove outliers and avoid skewing. \n",
    "\n",
    "\n",
    "\n",
    "**Summary of Data Inspection:**\n",
    "\n",
    "The summary statistics reveal a few important patterns and some clear anomalies. For example, `lead_time` appears highly consistent, with the 25th, 50th, and 75th percentiles all at 28 days which suggests a standardized process. Furhtermore, `unit_price` is tightly clustered between approximately $981 and $1004, which indicates price stability across most of the records. However, the minimum value of –$994 is possibly invalid and may indicate a misrecorded transaction or a potential return/refund. Additionnally, `order_quantity` shows a reasonable distribution centered around 80 units, however, the presence of a –119 minimum suggests a possible data issue, possibly related to returns or data entry errors. Lastly, the data needs to be inspected to ensure there are no duplicate entries. Overall, while the data appears generally well-structured but the extreme outliers, missing, and negative values should be flagged and addressed prior to proceeding with deeper analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde67d45",
   "metadata": {},
   "source": [
    "### Handling `NaN` Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6681148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop na's for sku_number\n",
    "\n",
    "df = df.dropna(subset=['sku_number'])\n",
    "\n",
    "# Drop rows where sku_numner column contains the string \"NA\"\n",
    "df = df[~df[['sku_number']].isin(['NA']).any(axis=1)]\n",
    "\n",
    "# handling negative unit_price values by converting to absolute values & filtering out negative order_quantity values\n",
    "\n",
    "df['unit_price'] = df['unit_price'].abs()\n",
    "df = df[df['order_quantity'] >= 0]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b1025e",
   "metadata": {},
   "source": [
    "`NaN` Handling Notes:\n",
    "\n",
    "- Removed incomplete records by dropping rows with missing (NaN) values or placeholder \"NA\" strings in `sku_number` column to ensure data integrity, reliability, and accuracy for categorization and future analyses.\n",
    "\n",
    "- Filtered outliers/ unusually high or low values in `unit_price` and `order_quantity` to avoid misleading results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf725bf",
   "metadata": {},
   "source": [
    " ### Useful Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaa772c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in 'sku_number' column: 473\n"
     ]
    }
   ],
   "source": [
    "# Determine unique sku_number count\n",
    "\n",
    "sku_count = df['sku_number'].nunique()\n",
    "print(f\"Number of unique values in 'sku_number' column: {sku_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad02f363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in 'manufacturing_site' column: 15\n"
     ]
    }
   ],
   "source": [
    "# Determine unique manufacturing_site count\n",
    "\n",
    "manufacturing_count = df['manufacturing_site'].nunique()\n",
    "print(f\"Number of unique values in 'manufacturing_site' column: {manufacturing_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa5d77fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in 'division_code' column: 66\n"
     ]
    }
   ],
   "source": [
    "# Determine division_code count\n",
    "\n",
    "division_count = df['division_code'].nunique()\n",
    "print(f\"Number of unique values in 'division_code' column: {division_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb80cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 transactions by order_quantity\n",
    "\n",
    "top_orders = df.nlargest(10, 'order_quantity')\n",
    "top_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def24ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottom 10 transactions by order_quantity\n",
    "\n",
    "bottom_orders = df.nsmallest(10, 'order_quantity')\n",
    "bottom_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ede3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 transactions by total_sales_value\n",
    "\n",
    "# create total_sales_value column and calculate transactions\n",
    "\n",
    "df['total_sales_value'] = (df['unit_price'] * df['order_quantity'])\n",
    "\n",
    "# Calculate and display top 10 sales transactions\n",
    "\n",
    "top_sales = df.sort_values(by='total_sales_value', ascending=False).head(10)\n",
    "top_sales.style.format({'total_sales_value': '${:,.2f}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c590a850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottom 10 transactions by total_sales_value\n",
    "\n",
    "bottom_sales = df.sort_values(by='total_sales_value', ascending=True).head(10).round(2)\n",
    "bottom_sales.style.format({'total_sales_value': '${:,.2f}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79de5ade",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "\n",
    "\n",
    "1.\tGeeksforGeeks – Working with Missing Data in Pandas\n",
    "GeeksforGeeks. “Working with Missing Data in Pandas.” GeeksforGeeks, 28 July 2025, https://www.geeksforgeeks.org/data-analysis/working-with-missing-data-in-pandas/.\n",
    "\n",
    "2.\tGeeksforGeeks – Pandas: How to Use dropna() with Specific Columns\n",
    "GeeksforGeeks. “Pandas DataFrame.dropna() Method.” GeeksforGeeks, 25 June 2025, https://www.geeksforgeeks.org/python/python-pandas-dataframe-dropna/.\n",
    "\n",
    "3.\tGeeksforGeeks – How to Find Duplicates in Pandas DataFrame (With Examples)\n",
    "GeeksforGeeks. “Find Duplicate Rows in a Dataframe Based on All or Selected Columns.” GeeksforGeeks, 4 Dec. 2023, https://www.geeksforgeeks.org/python/find-duplicate-rows-in-a-dataframe-based-on-all-or-selected-columns/.\n",
    "\n",
    "4.\tGeeksforGeeks – Filter Pandas Dataframe with Multiple Conditions\n",
    "GeeksforGeeks. “Filter Pandas Dataframe with Multiple Conditions.” GeeksforGeeks, 23 July 2025, https://www.geeksforgeeks.org/python/filter-pandas-dataframe-with-multiple-conditions/.\n",
    "\n",
    "5.\tGeeksforGeeks – Adding New Column to Existing DataFrame in Pandas\n",
    "GeeksforGeeks. “Adding New Column to Existing DataFrame in Pandas.” GeeksforGeeks, 11 July 2025, https://www.geeksforgeeks.org/pandas/adding-new-column-to-existing-dataframe-in-pandas/.\n",
    "\n",
    "6. GeeksforGeeks. Get n-Smallest Values from a Particular Column in Pandas DataFrame. GeeksforGeeks, 11 July 2025, https://www.geeksforgeeks.org/python/get-n-smallest-values-from-a-particular-column-in-pandas-dataframe/.\n",
    "\n",
    "7.\tPandas Documentation – Working with Missing Data\n",
    "Pandas Development Team. “Working with Missing Data.” Pandas 2.3.3 Documentation, pandas.pydata.org, https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html.\n",
    "\n",
    "8.\tStackoverflow. (2012). \"Renaming column names in Pandas\". Retrieved October 1st, 2025. https://stackoverflow.com/questions/11346283/renaming-column-names-in-pandas\n",
    "\n",
    "9.\tStackoverflow. (2012). \"How to show all columns' names on a large pandas dataframe?Retrieved September 30th, 2025. https://stackoverflow.com/questions/11346283/renaming-column-names-in-pandas\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1404116",
   "metadata": {},
   "source": [
    "## Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b71c4293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up shell for data and packages\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "metadata = pd.read_csv('../data/meta_data.csv')\n",
    "df = pd.read_csv('../data/transaction_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1951dbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe Formatting \n",
    "\n",
    "# Renaming columns for \"transaction\" dataframe\n",
    "\n",
    "df.rename(columns={'sku_numb3r': 'sku_number','leadTime': 'lead_time','Division Code':'division_code','Order-Quantity':'order_quantity'}, inplace=True)\n",
    "\n",
    "# drop na's for sku_number\n",
    "\n",
    "df = df.dropna(subset=['sku_number'])\n",
    "\n",
    "# handling negative unit_price values by converting to absolute values & filtering out negative order_quantity values\n",
    "\n",
    "df['unit_price'] = df['unit_price'].abs()\n",
    "df = df[df['order_quantity'] >= 0]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb79672",
   "metadata": {},
   "source": [
    "### Required Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73efbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for appropriate SKUs\n",
    "\n",
    "filtered_df = df[(df['inventory_type'] == 'FG') & (df['stocking_type'] == 'MTS')]\n",
    "grouped_df = filtered_df.groupby('sku_number')\n",
    "grouped_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb26db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aggregate order quantity stats per SKU group\n",
    "\n",
    "aggregate_df = grouped_df.aggregate({'order_quantity': ['min', 'max', 'mean', 'median', 'var', 'std'],'lead_time': 'mean'})\n",
    "aggregate_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2298bd05",
   "metadata": {},
   "source": [
    "### Safety Stock Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c09b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the safety stock for each SKU for the service level of 75%\n",
    "\n",
    "# Compute the value at the 75th percentile\n",
    "a75 = 0.75      \n",
    "alpha_75 = norm.ppf(a75)\n",
    "\n",
    "# calculate safety stock \n",
    "aggregate_df['safety_stock_75'] = (\n",
    "    alpha_75 * aggregate_df[('order_quantity', 'std')] * np.sqrt(aggregate_df[('lead_time', 'mean')])\n",
    ")\n",
    "\n",
    "\n",
    "# Compute the value at the 90th percentile\n",
    "a90 = 0.90\n",
    "alpha_90 = norm.ppf(a90)\n",
    "\n",
    "# calculate safety stock \n",
    "\n",
    "aggregate_df['safety_stock_90'] = (\n",
    "    alpha_90 * aggregate_df[('order_quantity', 'std')] * np.sqrt((aggregate_df[('lead_time', 'mean')])))\n",
    "\n",
    "\n",
    "\n",
    "# Compute the value at the 95th percentile\n",
    "a95 = 0.95\n",
    "alpha_95 = norm.ppf(a95)\n",
    "\n",
    "# calculate safety stock \n",
    "\n",
    "aggregate_df['safety_stock_95'] = (\n",
    "    alpha_95 * aggregate_df[('order_quantity', 'std')] * np.sqrt((aggregate_df[('lead_time', 'mean')])))\n",
    "\n",
    "rounded_aggregate_df = aggregate_df.apply(np.ceil).astype(int)\n",
    "rounded_aggregate_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa9d797",
   "metadata": {},
   "source": [
    "NOTES: added the`.apply(np.ceil).astype(int)` function to round safety stock calculation to the nearest whole number because you cant have partial units of inventory, however this doesn't provide the most accurate measure of safety stock because some are rounded up and some are rounded down (depending on the decimal point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26414e33",
   "metadata": {},
   "source": [
    "### Safety Stock Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b8a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which SKU has the largest safety stock at 95% service level\n",
    "max_safety_stock_95 = aggregate_df['safety_stock_95'].idxmax()\n",
    "print(\"SKU with the largest safety stock at the 95th percentile:\", max_safety_stock_95)\n",
    "\n",
    "\n",
    "# Determine which SKU has the smallest safety stock at 95% service level\n",
    "min_safety_stock_95 = aggregate_df['safety_stock_95'].idxmin()\n",
    "print(\"SKU with the smallest safety stock at the 95th percentile:\", min_safety_stock_95)\n",
    "\n",
    "# Calculate average safety stock across all SKUs at 95% service level\n",
    "average_safety_stock_95 = aggregate_df['safety_stock_95'].mean().round().astype(int)\n",
    "print(\"Average safety stock at the 95th percentile:\", average_safety_stock_95)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8574c229",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* Bobbitt, Zach. “Pandas: How to Use dropna() with Specific Columns.” Statology, 13 Feb. 2023, https://www.statology.org/pandas-dropna-specific-column/.\n",
    "\n",
    "\n",
    "* Kumar, Bijay. “Scipy Stats Zscore: Calculate and Use Z-Score.” *Python Guides*, 20 June 2025, https://pythonguides.com/scipy-stats-zscore/. Accessed 21 Oct. 2025.\n",
    "\n",
    "* NumPy Developers. “numpy.round — NumPy v2.3 Manual.” *NumPy*, https://numpy.org/doc/stable/reference/generated/numpy.round.html. Accessed 21 Oct. 2025.\n",
    "\n",
    "* pandas.DataFrame.aggregate — pandas 2.3.3 Documentation.” *pandas*, PyData, https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.aggregate.html.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
